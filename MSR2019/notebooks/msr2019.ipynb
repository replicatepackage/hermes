{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dataset for \"A Manually-Curated Dataset of Fixes to Vulnerabilities of Open-Source Software\" MSR 2019 (data showcase paper)\n",
    "\n",
    "This notebook is the companion to the paper **\"A Manually-Curated Dataset of Fixes to Vulnerabilities of Open-Source Software\"** by *Serena E. Ponta, Henrik Plate, Antonino Sabetta, Michele Bezzi, CÃ©dric Dangremont* from SAP Security Research.\n",
    "\n",
    "A pre-print is available at https://arxiv.org/abs/1902.02595."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This points to the dataset CSV file (leave out the '_release.csv' suffix!)\n",
    "VULAS_DB_NAME='../dataset/vulas_db_msr2019'\n",
    "\n",
    "GIT_REPO_FOLDER = '/tmp/git-cache-3'\n",
    "SKIP_CLONE_EXISTING=True\n",
    "\n",
    "# This controls how many negatives you want per each positive\n",
    "RATIO_POS_NEG = 1\n",
    "DATASET_NAME='dataset_msr2019'\n",
    "\n",
    "# Set random seed for reproducibility (used when creating 'negative' complement of the dataset)\n",
    "import random\n",
    "RND_SEED=17534532\n",
    "random.seed(RND_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys; sys.path.append(os.path.join(sys.path[0],'acacia'))\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import requests_cache\n",
    "requests_cache.install_cache('requests_cache', expire_after= 30 * 24*60*60)\n",
    "\n",
    "# acacia stuff\n",
    "from utils import *\n",
    "from acacia.git import clone_repo, clone_repo_multiple, extract_commit_msg, extract_commit_diff, get_random_commits\n",
    "from acacia.git_temporal import extract_timing_data\n",
    "\n",
    "POS_CLS='pos'\n",
    "NEG_CLS='neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LatexExporter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VULAS_DB_NAME + '_release.csv','r') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "vulas_db_content=[]\n",
    "for d in data:\n",
    "    line = d.strip().split(',')\n",
    "    vulas_db_content.append(tuple(line))\n",
    "\n",
    "# len(vulas_db_content)\n",
    "print('Dataset contains {} commits'.format(len(vulas_db_content)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloning\n",
    "\n",
    "This clones locally all (external git) repositories that appear in the Vulas DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repos_set = set([ r for _,r,_,_ in vulas_db_content ])\n",
    "clone_repo_multiple(repos_set,\n",
    "       output_folder=GIT_REPO_FOLDER,\n",
    "       proxy='',\n",
    "       skip_existing=SKIP_CLONE_EXISTING,\n",
    "       concurrent=8)\n",
    "len(repos_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing commit data in a dataframe (positive instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_commit(db_content, verbose=False):\n",
    "    ''' return a dataframe with commit data '''\n",
    "    data = []\n",
    "    for vuln_id, repo, commit, cls in tqdm(db_content):\n",
    "        try:\n",
    "            commit_msg = extract_commit_msg(commit, repo, GIT_REPO_FOLDER) \n",
    "            commit_diff = extract_commit_diff(commit, repo, GIT_REPO_FOLDER)\n",
    "            fix_tag, fix_tag_timestamp, commit_timestamp, commit_tag_delay = extract_timing_data(commit, repo, verbose, GIT_REPO_FOLDER)\n",
    "            data.append({\n",
    "                'msg' : commit_msg.decode(\"latin-1\"),\n",
    "                'diff': commit_diff.decode(\"latin-1\"),\n",
    "                'cls': cls,\n",
    "                'commit_id': commit,\n",
    "                'repo_url': repo,\n",
    "                'vuln_id': vuln_id,\n",
    "                'in_nvd': False,\n",
    "                'fix_tag': fix_tag,\n",
    "                'fix_tag_timestamp': int(fix_tag_timestamp),\n",
    "                'commit_timestamp' : int(commit_timestamp),\n",
    "                'commit_tag_delay': int(commit_tag_delay)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            print('Skipping %s' % repo)\n",
    "            print(e)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_pos = extract_commit(vulas_db_content, verbose=False)\n",
    "df_pos.to_pickle(DATASET_NAME + '_pos.pd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.read_pickle(DATASET_NAME + '_pos.pd')\n",
    "if len(vulas_db_content) - len(df_pos.index) != 0:\n",
    "    print('There where ' + str(len(vulas_db_content)- len(df_pos.index))  + ' commits that could not be processed')\n",
    "print('Successfully processed ' + str(len(df_pos.index)) + ' (pos) commits.')\n",
    "df_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos['days']=df_pos[df_pos['commit_tag_delay'] > 0]['commit_tag_delay']/(3600*24)\n",
    "\n",
    "df1=df_pos[df_pos['commit_tag_delay'] > 0]['commit_tag_delay']/(3600*24)\n",
    "\n",
    " \n",
    "\n",
    "df2=df1.to_frame()\n",
    "\n",
    " \n",
    "\n",
    "le.save('FIXBELOWHANDREDDAYS', len(df2[(df2['commit_tag_delay']).round()<100]), 'Number of fix commits released in less than 100 days')\n",
    "\n",
    "le.save('FIXBELOWONEDAYS', (len(df2[(df2['commit_tag_delay']).round() <1 ]) ), 'Number of fix commits released in less than 1 days')\n",
    "\n",
    "le.save('FIXNOTRELEASED', len(df_pos[df_pos['days']==np.nan]), 'Number of fix commits not released')\n",
    "\n",
    " \n",
    "\n",
    "d = {'Days': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,50,100,500,1000,2000 ],\n",
    "     '# Commits': [len(df2[df2['commit_tag_delay'].round()<1]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=1])&len(df2[df2['commit_tag_delay'].round()<2]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=2])&len(df2[df2['commit_tag_delay'].round()<3]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=3])&len(df2[df2['commit_tag_delay'].round()<4]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=4])&len(df2[df2['commit_tag_delay'].round()<5]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=5])&len(df2[df2['commit_tag_delay'].round()<6]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=6])&len(df2[df2['commit_tag_delay'].round()<7]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=7])&len(df2[df2['commit_tag_delay'].round()<8]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=8])&len(df2[df2['commit_tag_delay'].round()<9]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=9])&len(df2[df2['commit_tag_delay'].round()<10]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=10])&len(df2[df2['commit_tag_delay'].round()<11]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=11])&len(df2[df2['commit_tag_delay'].round()<12]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=12])&len(df2[df2['commit_tag_delay'].round()<13]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=13])&len(df2[df2['commit_tag_delay'].round()<14]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=14])&len(df2[df2['commit_tag_delay'].round()<15]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=15])&len(df2[df2['commit_tag_delay'].round()<50]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=50])&len(df2[df2['commit_tag_delay'].round()<100]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=100])&len(df2[df2['commit_tag_delay'].round()<500]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=500])&len(df2[df2['commit_tag_delay'].round()<1000]),\n",
    "        len(df2[df2['commit_tag_delay'].round()>=1000])&len(df2[df2['commit_tag_delay'].round()<2000])\n",
    "]}\n",
    "\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "   \n",
    "\n",
    "with open('msr2019/tex/delays'+'.tex','w') as tf:\n",
    "\n",
    "        tf.write(df.to_latex(columns=['Days','# Commits'],index=False, column_format='lc'))\n",
    "\n",
    "   \n",
    "\n",
    " \n",
    "\n",
    "plt.savefig('msr2019/img/commitTagDelay'+'.pdf', dpi=200, bbox_inches='tight')\n",
    "matplotlib.pyplot.hist(df1.round(),bins=20,log=True,histtype='bar')\n",
    "matplotlib.pyplot.xlabel(\"Days\")\n",
    "matplotlib.pyplot.ylabel(\"# Commits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create deduplicated version of (pos) dataset\n",
    "\n",
    "Deduplication is based on the equality of commit messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count duplicates\n",
    "\n",
    "dup_idx = df_pos.duplicated(subset='msg')\n",
    "if(len(dup_idx.value_counts())>1):\n",
    "    dup_commit_count = dup_idx.value_counts()[1]\n",
    "print('There are {} duplicate commits out of {}'.format(dup_commit_count, len(df_pos.index)))\n",
    "\n",
    "le.save('DUPLICATECOMMITSCOUNT', dup_commit_count, 'Number of duplicate commits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_dedup = df_pos.drop_duplicates(subset='msg')\n",
    "df_pos[df_pos.duplicated(subset='msg')].vuln_id.nunique()\n",
    "# df_pos_dedup.vuln_id.nunique()\n",
    "df_pos[dup_idx].sort_values('msg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "le.save('DATASETSIZEPOS', len(df_pos.index), 'Number of \\'pos\\' commits in the dataset (this includes duplicates!)' )\n",
    "le.save('DATASETSIZEPOSDEDUP', len(df_pos_dedup), 'Number of pos commits, **de-duplicated**')\n",
    "# le.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand dataset with 'negative' instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from acacia.git import *\n",
    "# # from acacia.dataset import *\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def expand_dataset(repos_set, df_pos):\n",
    "    neg_instances = []\n",
    "    SEC_REGEXP = '(denial.of.service|XXE|remote.code.execution|open.redirect|osvdb|secur.+|vuln.+|exploit.+|malicious.*|attack.*|dos|directory.traversal|injection|cve-\\d+-\\d+|xss|nvd|cross.site|csrf|rce|clickjack|session.fixation|advisory|insecur.+|cross.origin|unauthori[z|s].+)'\n",
    "\n",
    "    for repo in tqdm(repos_set):\n",
    "        count = len(df_pos.loc[df_pos['repo_url'] == repo])\n",
    "        commits = get_random_commits(int(count * RATIO_POS_NEG), repo, GIT_REPO_FOLDER)\n",
    "        for c in commits:\n",
    "            try:\n",
    "                new_instance = {}\n",
    "                commit_msg = extract_commit_msg(c, repo, GIT_REPO_FOLDER)\n",
    "                commit_diff = extract_commit_diff(c, repo, GIT_REPO_FOLDER) \n",
    "                if re.match(SEC_REGEXP, commit_msg.decode(\"utf-8\")):\n",
    "                    print('Found match of SEC_REGEXP in NEG instance')\n",
    "                    print(commit_msg)\n",
    "\n",
    "                new_instance = {\n",
    "                    'msg' : commit_msg.decode(\"latin-1\") ,\n",
    "                    'diff': commit_diff.decode(\"latin-1\") ,\n",
    "                    'cls': 'neg',\n",
    "                    'commit_id': c,\n",
    "                    'repo_url': repo ,\n",
    "                    'vuln_id' : '',\n",
    "                    'in_nvd': False,\n",
    "                    'fix_tag': '',\n",
    "                    'fix_tag_timestamp': 0,\n",
    "                    'commit_timestamp' : 0,\n",
    "                    'commit_tag_delay': 0\n",
    "                }\n",
    "                neg_instances.append(new_instance)\n",
    "            except Exception as e:\n",
    "                print('There was an exception with this commit, skipping: %s:%s' % (repo,c) )\n",
    "                print(e)\n",
    "    return pd.DataFrame(neg_instances)\n",
    "\n",
    "df_neg = expand_dataset(repos_set,df_pos_dedup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_pos_dedup, df_neg], ignore_index = True, sort=False)\n",
    "overall_size = len(df_all.index)\n",
    "print('The dataset contains '+ str(overall_size) + ' commits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_pickle(DATASET_NAME + '_all.pd')\n",
    "df_pos.to_pickle(DATASET_NAME + '_pos.pd')\n",
    "df_pos_dedup.to_pickle(DATASET_NAME + '_pos_dedup.pd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[['repo_url','commit_id','cls']].to_csv(DATASET_NAME + '.csv')\n",
    "df_pos_dedup[['repo_url','commit_id','cls']].to_csv(DATASET_NAME + '_pos_dedup.csv')\n",
    "df_pos[['repo_url','commit_id','cls']].to_csv(DATASET_NAME + '_pos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.read_pickle(DATASET_NAME + '_pos.pd')\n",
    "df_pos_dedup = pd.read_pickle(DATASET_NAME + '_pos_dedup.pd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def diff(first, second):\n",
    "        second = set(second)\n",
    "        return [item for item in first if item not in second]\n",
    "\n",
    "le.save('VULNLOSSDEDUP',\n",
    "            len(diff(set(df_pos['vuln_id'].values),set(df_pos_dedup['vuln_id'].values))),\n",
    "            'Number of vulnerabilities removed by the deduplication as they have the same fix commits than other vulnerabilities ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_counts(df, suffix=''):\n",
    "    print(\"Dataset size:           {}\".format(len(df.index)))\n",
    "    print(\"Instances of class POS: {}\".format(len(df[df['cls'] == POS_CLS ])))\n",
    "    print(\"Instances of class NEG: {}\".format(len(df[df['cls'] == NEG_CLS ])))\n",
    "\n",
    "    le.save('COMMITCOUNT' + suffix,\n",
    "            len(df[df['cls'] == POS_CLS ]),\n",
    "            'Number of commits ' + suffix)\n",
    "    \n",
    "    le.save('VULNCOUNT'+suffix,\n",
    "            df[df['cls'] == POS_CLS ].vuln_id.nunique(),\n",
    "            'Unique vulnerabilities (after mapping vulns) ' + suffix)\n",
    "\n",
    "    le.save('REPOCOUNT'+suffix,\n",
    "            df[df['cls'] == POS_CLS ].repo_url.nunique(),\n",
    "            'Number of unique repositories (after removing internal, dead, svn). ' + suffix)\n",
    "    \n",
    "    df.head()\n",
    "\n",
    "print_counts(df_pos)\n",
    "print('-----------------------------------------------')\n",
    "print_counts(df_pos_dedup,'DEDUP')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Commits per CVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printVulnXCommit(df,suffix=''):\n",
    "    df2 = df.loc[df['cls']==POS_CLS,['commit_id','vuln_id']]\n",
    "    df_vulnOnly = df2[['vuln_id']]\n",
    "    df_vulnOnly['commits'] = df_vulnOnly.groupby('vuln_id')['vuln_id'].transform('count')\n",
    "    df_vulnOnly=df_vulnOnly.drop_duplicates(subset=None, keep='first', inplace=False).sort_values(by='commits',ascending=False)\n",
    "    #print(df_vulnOnly.sort_values(by='commits',ascending=False).head(20))\n",
    "    print(\"Number of vuln with 1 commit {}: {}\".format(suffix, len(df_vulnOnly[df_vulnOnly['commits']==1])))\n",
    "\n",
    "\n",
    "    with open('msr2019/tex/commitXvuln'+suffix+'.tex','w') as tf:\n",
    "        tf.write(df_vulnOnly.to_latex(columns=['vuln_id','commits'],index=False, column_format='lc'))\n",
    "        \n",
    "    # Create figure\n",
    "    f, ax1 = plt.subplots(figsize=(5,3), dpi=200)\n",
    "\n",
    "    df_vulnOnly.groupby('commits').count().plot(ax=ax1, kind='bar',logy=True )\n",
    "\n",
    "    labels = df_vulnOnly.groupby('commits').count()\n",
    "#     print(labels['vuln_id'].tolist())\n",
    "    #for i, v in enumerate(labels['vuln_id'].tolist()):\n",
    "    #    ax1.text(v , i + .25, str(v), color='blue', fontweight='bold')\n",
    "    \n",
    "    #ax1.legend(loc=(0.65,0.05))\n",
    "    ax1.legend().set_visible(False)\n",
    "\n",
    "    f.subplots_adjust(hspace=0.1)\n",
    "    # plt.xticks([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "    ax1.grid(False)\n",
    "    # ax1.set_title('msg', x=0.92, y=0.05)\n",
    "    #ax1.set_title('# Vulnerabilities', x=0.52, y=0.85)\n",
    "    plt.ylabel('# Vulnerabilities')\n",
    "    plt.xlabel('# Commits')\n",
    "\n",
    "    plt.setp([a.get_xticklabels() for a in f.axes[:-1]], visible=False)\n",
    "    plt.savefig('msr2019/img/commitXvuln'+suffix+'.pdf', dpi=200, bbox_inches='tight')\n",
    "\n",
    "printVulnXCommit(df_pos)\n",
    "print('-----------------------------------------------')\n",
    "printVulnXCommit(df_pos_dedup,'DEDUP')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of vulnerabilities per repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printvulnXrepo(df,suffix=''):\n",
    "    df1=df.loc[df['cls']==POS_CLS,['repo_url','vuln_id']]\n",
    "\n",
    "    #df1.groupby('repo_url').vuln_id.nunique()\n",
    "    df1['distinct_vuln']=df1.groupby('repo_url')['vuln_id'].transform('nunique')\n",
    "    df1.sort_values(by=['repo_url','vuln_id'])\n",
    "\n",
    "\n",
    "\n",
    "    df1=df1[['repo_url','distinct_vuln']].drop_duplicates(subset=None, keep='first', inplace=False).sort_values(by='distinct_vuln',ascending=False)\n",
    "    #df1.plot()\n",
    "    #df1.groupby('distinct_vuln').count().plot(kind='bar')\n",
    "\n",
    "    with open('msr2019/tex/vulnXrepo'+suffix+'.tex','w') as tf:\n",
    "        tf.write(df1.to_latex(columns=['repo_url','distinct_vuln'],index=False, column_format='lc'))\n",
    "        # Create figure\n",
    "    f, ax1 = plt.subplots(figsize=(5,3), dpi=200)\n",
    "    #print(df1.groupby('distinct_vuln').count())\n",
    "    df1.groupby('distinct_vuln').count().plot(ax=ax1, kind='bar',logy=True )\n",
    "\n",
    "    #ax1.legend(loc=(0.65,0.05))\n",
    "    ax1.legend().set_visible(False)\n",
    "\n",
    "    f.subplots_adjust(hspace=0.1)\n",
    "    #plt.xticks([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "    ax1.grid(False)\n",
    "    # ax1.set_title('msg', x=0.92, y=0.05)\n",
    "    #ax1.set_title('# Repo', x=0.52, y=0.85)\n",
    "    plt.ylabel('#Repositories')\n",
    "    plt.xlabel('#Vulnerabilities')\n",
    "\n",
    "    plt.setp([a.get_xticklabels() for a in f.axes[:-1]], visible=False)\n",
    "\n",
    "    plt.savefig('msr2019/img/repoXNumVuln'+suffix+'.pdf', dpi=200, bbox_inches='tight')\n",
    "\n",
    "printvulnXrepo(df_pos)\n",
    "printvulnXrepo(df_pos_dedup,'DEDUP')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vulnerabilities per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getYear(vuln_id):\n",
    "    LOOKUP_TABLE_YEAR = {\n",
    "        \"413684\" : \"2013\",\n",
    "        \"SONARQUBE-001\" : \"2018\",\n",
    "        \"NIFI-4436\" : \"2017\",\n",
    "        \"HADOOP-14246\" : \"2017\",\n",
    "        \"HDFS-10276\" : \"2016\",\n",
    "        \"ND4J-001\" : \"2018\",\n",
    "        \"APACHE-COMMONS-001\" : \"2018\",\n",
    "        \"HADOOP-13105\" : \"2016\",\n",
    "        \"HADOOP-12001\" : \"2015\",\n",
    "        \"HADOOP-15212\" : \"2018\",\n",
    "        \"APACHE-AXIS2-5683\" : \"2015\",\n",
    "        \"S2-028\" : \"2017\",\n",
    "        \"PLEXUS-ARCHIVER-87\" : \"2018\",\n",
    "        \"HADOOP-12751\" : \"2016\",\n",
    "        \"JAVAMELODY-252\" : \"2015\",\n",
    "        \"PT-2013-65\" : \"2013\",\n",
    "        \"SPR-7779\" : \"2010\",\n",
    "        \"HTTPCLIENT-1803\" : \"2017\",\n",
    "        \"PRIMEFACES-1194\" : \"2016\",\n",
    "        \"JAVAMELODY-631\" : \"2017\",\n",
    "        \"ZEPPELIN-2769\" : \"2017\",\n",
    "        \"APACHE-AXIS2-5846\" : \"2017\",\n",
    "        \"AMQ-5751\" : \"2015\",\n",
    "        \"AMQP-590\": \"2016\",\n",
    "        \"2012-05-05\" : \"2012\",\n",
    "        \"HUDSON-483532\" : \"2015\",\n",
    "        \"GEODE-4270\" : \"2018\",\n",
    "        \"S2-043\" : \"2016\",\n",
    "        \"JETTY-1042\" : \"2009\",\n",
    "        \"COLLECTIONS-580\": '2015',\n",
    "        \"PDFBOX-3341\":\"2016\"\n",
    "    }\n",
    "    z = re.search(r'(?<=CVE-)\\d{4}',vuln_id)\n",
    "    if(z is not None):\n",
    "        year = z.group(0)\n",
    "    else:\n",
    "        year = LOOKUP_TABLE_YEAR[vuln_id]\n",
    "    return year\n",
    "\n",
    "def checkNVD(vuln_id):\n",
    "    url = 'https://nvd.nist.gov/vuln/detail/'+vuln_id\n",
    "#     print('Checking the NVD for ' + url)\n",
    "    r = requests.get('https://nvd.nist.gov/vuln/detail/'+vuln_id)\n",
    "    if r.status_code!=200 :\n",
    "        return False\n",
    "    if 'CVE ID Not Found' in r.text:\n",
    "        return False\n",
    "    if 'Vuln ID, expected format: CVE-'  in r.text:\n",
    "        return False\n",
    "    return True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printVulnXYear(df,suffix=''):\n",
    "    df_vuln=df.loc[df['cls']==POS_CLS,['vuln_id']].drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "\n",
    "    print(\"Number of distinct vuln {} : {}\".format(suffix, len(df_vuln)))\n",
    "\n",
    "    df_vuln['year'] = df_vuln['vuln_id'].apply(lambda x: getYear(x))\n",
    "\n",
    "    # Create figure\n",
    "    f, ax1 = plt.subplots(figsize=(5,4), dpi=200)\n",
    "\n",
    "    df_vuln.groupby('year').count().plot(kind='bar',ax=ax1)\n",
    "\n",
    "    #ax1.legend(loc=(0.65,0.05))\n",
    "    ax1.legend().set_visible(False)\n",
    "\n",
    "    f.subplots_adjust(hspace=0.1)\n",
    "    #plt.xticks([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "    ax1.grid(False)\n",
    "    # ax1.set_title('msg', x=0.92, y=0.05)\n",
    "    #ax1.set_title('# Repo', x=0.52, y=0.85)\n",
    "    plt.ylabel('#Vulnerabilities')\n",
    "\n",
    "    plt.setp([a.get_xticklabels() for a in f.axes[:-1]], visible=False)\n",
    "\n",
    "    plt.savefig('msr2019/img/vulnXYear'+suffix+'.pdf', dpi=200, bbox_inches='tight')\n",
    "    \n",
    "printVulnXYear(df_pos)\n",
    "printVulnXYear(df_pos_dedup,'DEDUP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populateInNvd(df):\n",
    "    count=0\n",
    "    for i, row in df.iterrows():\n",
    "        #print('https://nvd.nist.gov/vuln/detail/'+x)\n",
    "        count += 1\n",
    "        print('.', end = '')\n",
    "        if not count % 50:\n",
    "            print(' ' + str(count) + '\\n')\n",
    "        check_outcome = checkNVD(df.at[i,'vuln_id'])\n",
    "        df.at[i,'in_nvd'] = check_outcome\n",
    "    print('\\n')\n",
    "        \n",
    "populateInNvd(df_pos)\n",
    "populateInNvd(df_pos_dedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countNVDStats(df,suffix=''):\n",
    "    vulns_all_count = df.vuln_id.nunique()\n",
    "    le.save('ALLVULNSCOUNT' +suffix,\n",
    "            vulns_all_count,\n",
    "            \"Number of distinct vulnerabilities in the dataset \" + suffix)\n",
    "\n",
    "    vulns_nvd_count = df[df['in_nvd'] == 1.0 ].vuln_id.nunique()\n",
    "    le.save('ALLVULNSINNVD'+suffix,\n",
    "        vulns_nvd_count,\n",
    "        'Number of vulnerabilities found in the NVD '  + suffix)\n",
    "\n",
    "    cves_no_nvd_count = vulns_all_count - vulns_nvd_count\n",
    "    le.save('CVENONVD'+suffix,\n",
    "        cves_no_nvd_count,\n",
    "        'Number of vulnerabilities that have a CVE but are not found in the NVD '  + suffix)\n",
    "\n",
    "    cves_count = len(df[df['vuln_id'].str.contains('CVE-')]['vuln_id'].unique())\n",
    "    le.save('VULNSNOCVE'+suffix,\n",
    "        vulns_all_count - cves_count,\n",
    "        'Number of vulnerabilities that do not have a CVE name '  + suffix)\n",
    "    \n",
    "countNVDStats(df_pos)\n",
    "countNVDStats(df_pos_dedup,'DEDUP')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
